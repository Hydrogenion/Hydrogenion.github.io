<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="sklearn," />










<meta name="description" content="这一篇虽然叫做：十分钟上手sklearn：特征提取，常用模型，但是写着写着我就想把每一个模型都详细说一下，所以也可以看作是机器学习算法概述了。
上一篇我们讲解了如何安装sklearn,导入自带数据集，创建数据，对数据进行预处理，通过上一篇的讲解，相信大家能够感受到sklearn的强大之处。这一篇，我们将对sklearn中有关特征提取，常用模型进行讲解。主要内容包括：1.PCA算法2.LDA算法3.">
<meta property="og:type" content="article">
<meta property="og:title" content="十分钟上手sklearn：特征提取，常用模型，交叉验证">
<meta property="og:url" content="http://blackblog.tech/2018/02/05/十分钟上手sklearn-2/index.html">
<meta property="og:site_name" content="Black'Blog">
<meta property="og:description" content="这一篇虽然叫做：十分钟上手sklearn：特征提取，常用模型，但是写着写着我就想把每一个模型都详细说一下，所以也可以看作是机器学习算法概述了。
上一篇我们讲解了如何安装sklearn,导入自带数据集，创建数据，对数据进行预处理，通过上一篇的讲解，相信大家能够感受到sklearn的强大之处。这一篇，我们将对sklearn中有关特征提取，常用模型进行讲解。主要内容包括：1.PCA算法2.LDA算法3.">
<meta property="og:image" content="https://blog-1253353217.cos.ap-chengdu.myqcloud.com/blog4%20sklearn/sklearn1.png">
<meta property="og:image" content="https://blog-1253353217.cos.ap-chengdu.myqcloud.com/blog4%20sklearn/sklearn2.png">
<meta property="og:updated_time" content="2018-02-05T17:34:53.134Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="十分钟上手sklearn：特征提取，常用模型，交叉验证">
<meta name="twitter:description" content="这一篇虽然叫做：十分钟上手sklearn：特征提取，常用模型，但是写着写着我就想把每一个模型都详细说一下，所以也可以看作是机器学习算法概述了。
上一篇我们讲解了如何安装sklearn,导入自带数据集，创建数据，对数据进行预处理，通过上一篇的讲解，相信大家能够感受到sklearn的强大之处。这一篇，我们将对sklearn中有关特征提取，常用模型进行讲解。主要内容包括：1.PCA算法2.LDA算法3.">
<meta name="twitter:image" content="https://blog-1253353217.cos.ap-chengdu.myqcloud.com/blog4%20sklearn/sklearn1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://blackblog.tech/2018/02/05/十分钟上手sklearn-2/"/>





  <title>十分钟上手sklearn：特征提取，常用模型，交叉验证 | Black'Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Black'Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Code Tech Life</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blackblog.tech/2018/02/05/十分钟上手sklearn-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Black'Blog">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Black'Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">十分钟上手sklearn：特征提取，常用模型，交叉验证</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-05T23:16:34+08:00">
                2018-02-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/05/十分钟上手sklearn-2/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/02/05/十分钟上手sklearn-2/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/02/05/十分钟上手sklearn-2/" class="leancloud_visitors" data-flag-title="十分钟上手sklearn：特征提取，常用模型，交叉验证">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>这一篇虽然叫做：十分钟上手sklearn：特征提取，常用模型，但是写着写着我就想把每一个模型都详细说一下，所以也可以看作是机器学习算法概述了。</p>
<p>上一篇我们讲解了如何安装sklearn,导入自带数据集，创建数据，对数据进行预处理，通过上一篇的讲解，相信大家能够感受到sklearn的强大之处。<br>这一篇，我们将对sklearn中有关特征提取，常用模型进行讲解。<br>主要内容包括：<br>1.PCA算法<br>2.LDA算法<br>3.线性回归<br>4.逻辑回归<br>5.朴素贝叶斯<br>6.决策树<br>7.SVM<br>8.神经网络<br>9.KNN算法<br>是不是感觉干货满满啊！Let’s get moving!!!<br><a id="more"></a></p>
<h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>我们获取的数据中很多数据往往有很多维度，但并不是所有的维度都是有用的，有意义的，所以我们要将对结果影响较小的维度舍去，保留对结果影响较大的维度。<br>PCA（主成分分析）与LDA（线性评价分析）是特征提取的两种经典算法。PCA与LDA本质上都是学习一个投影矩阵，使样本在新的坐标系上的表示具有相应的特性，样本在新坐标系的坐标相当于新的特征，保留下的新特征应当是对结果有较大影响的特征。</p>
<h3 id="PCA（主成分分析）"><a href="#PCA（主成分分析）" class="headerlink" title="PCA（主成分分析）"></a>PCA（主成分分析）</h3><p>最大方差理论：信号具有较大的方差，噪声具有较小的方差<br>PCA的目标：新坐标系上数据的方差越大越好<br>PCA是无监督的学习方法<br>PCA实现起来并不复杂（过几天写一篇使用NumPy实现的PCA），但是在sklearn就更为简单了，直接食用skleran.decomposition即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.decomposition <span class="keyword">as</span> sk_decomposition</div><div class="line">pca = sk_decomposition.PCA(n_components=<span class="string">'mle'</span>,whiten=<span class="keyword">False</span>,svd_solver=<span class="string">'auto'</span>)</div><div class="line">pca.fit(iris_X)</div><div class="line">reduced_X = pca.transform(iris_X) <span class="comment">#reduced_X为降维后的数据</span></div><div class="line">print(<span class="string">'PCA:'</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">'降维后的各主成分的方差值占总方差值的比例'</span>,pca.explained_variance_ratio_)</div><div class="line"><span class="keyword">print</span> (<span class="string">'降维后的各主成分的方差值'</span>,pca.explained_variance_)</div><div class="line"><span class="keyword">print</span> (<span class="string">'降维后的特征数'</span>,pca.n_components_)</div></pre></td></tr></table></figure>
<p>参数说明：<br>n_components：指定希望PCA降维后的特征维度数目(&gt;1)， 指定主成分的方差和所占的最小比例阈值（0-1），’mle’用MLE算法根据特征的方差分布情况自己去选择一定数量的主成分特征来降维<br>    whiten： 判断是否进行白化。白化：降维后的数据的每个特征进行归一化，让方差都为1<br>    svd_solver：奇异值分解SVD的方法{‘auto’, ‘full’, ‘arpack’, ‘randomized’}</p>
<p>打印结果:<br>下面打印的内容只是帮助大家理解pca的参数，就不打印降维后的数据了，打印出来并没有什么意义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">PCA:</div><div class="line">降维后的各主成分的方差值占总方差值的比例 [ <span class="number">0.92461621</span>  <span class="number">0.05301557</span>  <span class="number">0.01718514</span>]</div><div class="line">降维后的各主成分的方差值 [ <span class="number">4.22484077</span>  <span class="number">0.24224357</span>  <span class="number">0.07852391</span>]</div><div class="line">降维后的特征数 <span class="number">3</span></div></pre></td></tr></table></figure>
<h3 id="LDA（线性评价分析）"><a href="#LDA（线性评价分析）" class="headerlink" title="LDA（线性评价分析）"></a>LDA（线性评价分析）</h3><p>LDA基于费舍尔准则，即同一类样本尽可能聚合在一起，不同类样本应该尽量扩散；或者说，同雷洋被具有较好的聚合度，类别间具有较好的扩散度。<br>既然涉及到了类别，那么LDA肯定是一个有监督算法，其实LDA既可以做特征提取液可以做分类。<br>LDA具体的实现流程这里就不再赘述了，直接看skleran如何实现LDA。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.discriminant_analysis <span class="keyword">as</span> sk_discriminant_analysis</div><div class="line">lda = sk_discriminant_analysis.LinearDiscriminantAnalysis(n_components=<span class="number">2</span>)</div><div class="line">lda.fit(iris_X,iris_y)</div><div class="line">reduced_X = lda.transform(iris_X) <span class="comment">#reduced_X为降维后的数据</span></div><div class="line">print(<span class="string">'LDA:'</span>)</div><div class="line"><span class="keyword">print</span> (<span class="string">'LDA的数据中心点:'</span>,lda.means_) <span class="comment">#中心点</span></div><div class="line"><span class="keyword">print</span> (<span class="string">'LDA做分类时的正确率:'</span>,lda.score(X_test, y_test)) <span class="comment">#score是指分类的正确率</span></div><div class="line"><span class="keyword">print</span> (<span class="string">'LDA降维后特征空间的类中心:'</span>,lda.scalings_) <span class="comment">#降维后特征空间的类中心</span></div></pre></td></tr></table></figure>
<p>参数说明：<br>    n_components：指定希望PCA降维后的特征维度数目(&gt;1)<br>    svd_solver：奇异值分解SVD的方法{‘auto’, ‘full’, ‘arpack’, ‘randomized’}</p>
<p>打印结果:<br>下面打印的内容只是帮助大家理解lda的参数，就不打印降维后的数据了，打印出来并没有什么意义。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">LDA:</div><div class="line">LDA的数据中心点: </div><div class="line">[[ <span class="number">5.006</span>  <span class="number">3.418</span>  <span class="number">1.464</span>  <span class="number">0.244</span>]</div><div class="line"> [ <span class="number">5.936</span>  <span class="number">2.77</span>   <span class="number">4.26</span>   <span class="number">1.326</span>]</div><div class="line"> [ <span class="number">6.588</span>  <span class="number">2.974</span>  <span class="number">5.552</span>  <span class="number">2.026</span>]]</div><div class="line">LDA做分类时的正确率: <span class="number">0.980952380952</span></div><div class="line">LDA降维后特征空间的类中心: </div><div class="line">[[<span class="number">-0.81926852</span>  <span class="number">0.03285975</span>]</div><div class="line"> [<span class="number">-1.5478732</span>   <span class="number">2.15471106</span>]</div><div class="line"> [ <span class="number">2.18494056</span> <span class="number">-0.93024679</span>]</div><div class="line"> [ <span class="number">2.85385002</span>  <span class="number">2.8060046</span> ]]</div></pre></td></tr></table></figure>
<h2 id="常用模型"><a href="#常用模型" class="headerlink" title="常用模型"></a>常用模型</h2><p>好了，好了，终于可以开始讲模型了，其实这才是我想讲的重点啊，没想到前面的内容都讲了这么多。。。<br>机器学习常用的算法也就那几个，sklearn中对其都做了实现，我们只需要调用即可。下面每一个算法的原理我就不细讲了，只讲怎么用，以后会写这些算法的具体原理与实现方式。<br>干货要来了，准备好！</p>
<p>首先sklearn中所有的模型都有四个固定且常用的方法，其实在PCA与LDA中我们已经用到了这些方法中的fit方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 拟合模型</span></div><div class="line">model.fit(X_train, y_train)</div><div class="line"><span class="comment"># 模型预测</span></div><div class="line">model.predict(X_test)</div><div class="line"><span class="comment"># 获得这个模型的参数</span></div><div class="line">model.get_params()</div><div class="line"><span class="comment"># 为模型进行打分</span></div><div class="line">model.score(data_X, data_y) <span class="comment"># 回归问题：以R2参数为标准 分类问题：以准确率为标准</span></div></pre></td></tr></table></figure>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w’x+e，e为误差服从均值为0的正态分布。<br>回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。<br>其实，说白了，就是用一条直线去拟合一大堆数据，最后把系数w和截距b算出来，直线也就算出来了， 就可以拿去做预测了。<br>sklearn中线性回归使用最小二乘法实现，使用起来非常简单。<br>线性回归是回归问题，score使用R2系数做为评价标准。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.linear_model <span class="keyword">as</span> sk_linear</div><div class="line">model = sk_linear.LinearRegression(fit_intercept=<span class="keyword">True</span>,normalize=<span class="keyword">False</span>,copy_X=<span class="keyword">True</span>,n_jobs=<span class="number">1</span>)</div><div class="line">model.fit(X_train,y_train)</div><div class="line">acc=model.score(X_test,y_test) <span class="comment">#返回预测的确定系数R2</span></div><div class="line">print(<span class="string">'线性回归:'</span>)</div><div class="line">print(<span class="string">'截距:'</span>,model.intercept_) <span class="comment">#输出截距</span></div><div class="line">print(<span class="string">'系数:'</span>,model.coef_) <span class="comment">#输出系数</span></div><div class="line">print(<span class="string">'线性回归模型评价:'</span>,acc)</div></pre></td></tr></table></figure>
<p>参数说明：<br>    fit_intercept：是否计算截距。False-模型没有截距<br>    normalize： 当fit_intercept设置为False时，该参数将被忽略。 如果为真，则回归前的回归系数X将通过减去平均值并除以l2-范数而归一化。<br>    copy_X：是否对X数组进行复制,默认为True<br>    n_jobs：指定线程数</p>
<p>打印结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">线性回归:</div><div class="line">截距: <span class="number">-0.379953866745</span></div><div class="line">系数: [<span class="number">-0.02744885</span>  <span class="number">0.01662843</span>  <span class="number">0.17780211</span>  <span class="number">0.65838886</span>]</div><div class="line">线性回归模型评价: <span class="number">0.913431360638</span></div></pre></td></tr></table></figure>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 w‘x+b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将w‘x+b作为因变量，即y =w‘x+b，而logistic回归则通过函数L将w‘x+b对应一个隐状态p，p =L(w‘x+b),然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归。<br>说人话：线性回归是回归，逻辑回归是分类。逻辑回归通过logistic函数算概率，然后算出来一个样本属于一个类别的概率，概率越大越可能是这个类的样本。<br>sklearn对于逻辑回归的实现也非常简单，直接上代码了。<br>逻辑回归是分类问题，score使用准确率做为评价标准。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.linear_model <span class="keyword">as</span> sk_linear</div><div class="line">model = sk_linear.LogisticRegression(penalty=<span class="string">'l2'</span>,dual=<span class="keyword">False</span>,C=<span class="number">1.0</span>,n_jobs=<span class="number">1</span>,random_state=<span class="number">20</span>,fit_intercept=<span class="keyword">True</span>)</div><div class="line">model.fit(X_train,y_train) <span class="comment">#对模型进行训练</span></div><div class="line">acc=model.score(X_test,y_test) <span class="comment">#根据给定数据与标签返回正确率的均值</span></div><div class="line">print(<span class="string">'逻辑回归模型评价:'</span>,acc)</div></pre></td></tr></table></figure></p>
<p>参数说明：<br>    penalty：使用指定正则化项（默认：l2）<br>    dual: n_samples &gt; n_features取False（默认）<br>    C：正则化强度的反，值越小正则化强度越大<br>    n_jobs: 指定线程数<br>    random_state：随机数生成器<br>    fit_intercept: 是否需要常量</p>
<p>打印结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">逻辑回归模型评价: <span class="number">0.8</span></div></pre></td></tr></table></figure>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。<br>而朴素朴素贝叶斯分类是贝叶斯分类中最简单，也是常见的一种分类方法<br>首先根据样本中心定理，概率等于频率，所以下文的P是可以统计出来的<br>朴素贝叶斯的核心便是贝叶斯公式：P(B|A)=P(A|B)P(B)/P(A) 即在A条件下，B发生的概率<br>换个角度：P(类别|特征)=P(特征|类别)P(类别)/P(特征)<br>而我们最后要求解的就是P(类别|特征)<br>举一个生活中的例子：<br><img src="https://blog-1253353217.cos.ap-chengdu.myqcloud.com/blog4%20sklearn/sklearn1.png" alt="贝叶斯"><br>最后一个公式中的所有概率都是可以统计出来的，所以P(B|A)可以计算！<br>那么！我感觉我都写偏题了，这明明是机器学习算法概述嘛<br>那么sklearn中怎么实现呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.naive_bayes <span class="keyword">as</span> sk_bayes</div><div class="line">model = sk_bayes.MultinomialNB(alpha=<span class="number">1.0</span>,fit_prior=<span class="keyword">True</span>,class_prior=<span class="keyword">None</span>) <span class="comment">#多项式分布的朴素贝叶斯</span></div><div class="line">model = sk_bayes.BernoulliNB(alpha=<span class="number">1.0</span>,binarize=<span class="number">0.0</span>,fit_prior=<span class="keyword">True</span>,class_prior=<span class="keyword">None</span>) <span class="comment">#伯努利分布的朴素贝叶斯</span></div><div class="line">model = sk_bayes.GaussianNB()<span class="comment">#高斯分布的朴素贝叶斯</span></div><div class="line">model.fit(X_train,y_train)</div><div class="line">acc=model.score(X_test,y_test) <span class="comment">#根据给定数据与标签返回正确率的均值</span></div><div class="line">print(n朴素贝叶斯(高斯分布)模型评价:<span class="string">',acc)</span></div></pre></td></tr></table></figure>
<p>参数说明：<br>    alpha：平滑参数<br>    fit_prior：是否要学习类的先验概率；false-使用统一的先验概率<br>    class_prior: 是否指定类的先验概率；若指定则不能根据参数调整<br>    binarize: 二值化的阈值，若为None，则假设输入由二进制向量组成</p>
<p>打印结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">朴素贝叶斯(高斯分布)模型评价: <span class="number">0.92380952381</span></div></pre></td></tr></table></figure>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>决策树是解决分类问题<br>算法描述请见我之前的帖子（写的很详细了）：<a href="http://blackblog.tech/2018/01/29/决策树——ID3算法实现/">http://blackblog.tech/2018/01/29/决策树——ID3算法实现/</a><br>这里我们直接上代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.tree <span class="keyword">as</span> sk_tree</div><div class="line">model = sk_tree.DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>,max_depth=<span class="keyword">None</span>,min_samples_split=<span class="number">2</span>,min_samples_leaf=<span class="number">1</span>,max_features=<span class="keyword">None</span>,max_leaf_nodes=<span class="keyword">None</span>,min_impurity_decrease=<span class="number">0</span>)</div><div class="line">model.fit(X_train,y_train)</div><div class="line">acc=model.score(X_test,y_test) <span class="comment">#根据给定数据与标签返回正确率的均值</span></div><div class="line">print(<span class="string">'决策树模型评价:'</span>,acc)</div></pre></td></tr></table></figure>
<p>参数说明：<br>    criterion ：特征选择准则gini/entropy<br>    max_depth：树的最大深度，None-尽量下分<br>    min_samples_split：分裂内部节点，所需要的最小样本树<br>    min_samples_leaf：叶子节点所需要的最小样本数<br>    max_features: 寻找最优分割点时的最大特征数<br>    max_leaf_nodes：优先增长到最大叶子节点数<br>    min_impurity_decrease：如果这种分离导致杂质的减少大于或等于这个值，则节点将被拆分。</p>
<p>打印结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">决策树模型评价: <span class="number">0.942857142857</span></div></pre></td></tr></table></figure>
<h3 id="SVM-支持向量机）"><a href="#SVM-支持向量机）" class="headerlink" title="SVM(支持向量机）"></a>SVM(支持向量机）</h3><p>支持向量机是解决分类问题<br>目的：求解最大化间隔<br>支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。建立方向合适的分隔超平面使两个与之平行的超平面间的距离最大化。其假定为，平行超平面间的距离或差距越大，分类器的总误差越小。<br>SVM的关键在于核函数<br>一句话讲懂核函数：低维无法线性划分的问题放到高维就可以线性划分，一般用高斯，因为效果绝对不会变差！<br>SVM算法思路很清晰，但是实现起来很复杂，最近就在实现SVM，写好了就发上来，在这里就不赘述这么多了，我们直接用skleran解决问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.svm <span class="keyword">as</span> sk_svm</div><div class="line">model = sk_svm.SVC(C=<span class="number">1.0</span>,kernel=<span class="string">'rbf'</span>,gamma=<span class="string">'auto'</span>)</div><div class="line">model.fit(X_train,y_train)</div><div class="line">acc=model.score(X_test,y_test) <span class="comment">#根据给定数据与标签返回正确率的均值</span></div><div class="line">print(<span class="string">'SVM模型评价:'</span>,acc)</div></pre></td></tr></table></figure>
<p>参数说明：<br>    C：误差项的惩罚参数C<br>    kernel：核函数选择 默认：rbf(高斯核函数)，可选：‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’<br>    gamma: 核相关系数。浮点数，If gamma is ‘auto’ then 1/n_features will be used instead.点将被拆分。</p>
<p>打印结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">SVM模型评价: <span class="number">0.961904761905</span></div></pre></td></tr></table></figure>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>还在感慨因为不会tensorflow而无法使用神经网络？还在羡慕神经网络的惊人效果?不需要tf，不需要caffe，不需要pytorch！只要一句话，便可以实现多层神经网络！！！<br>在这里还是简单说一下M-P神经元的原理：<br><img src="https://blog-1253353217.cos.ap-chengdu.myqcloud.com/blog4%20sklearn/sklearn2.png" alt="神经元"><br>𝒙𝒊 来自第𝑖个神经元的输入<br>𝒘𝒊 第𝑖个神经元的连接权重<br>𝜽 阈值(threshold)或称为偏置（bias）<br>𝑓 为激活函数，常用：sigmoid，relu，tanh等等<br>对于一个神经元来说，有i个输入，每一个输入都对应一个权重（w），神经元具有一个偏置（阈值），将所有的i*w求和后减去阈值得到一个值，这个值就是激活函数的参数，激活函数将根据这个参数来判定这个神经元是否被激活。<br>本质上, M-P神经元=线性二分类器<br>那么什么是多层神经网络？<br>线性不可分：一个超平面没法解决问题，就用两个超平面来解决，什么？还不行！那就再增加超平面直到解决问题为止。    ——多层神经网络<br>没错，多层神经元就是用来解决线性不可分问题的。<br>那么，sklearn中如何实现呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.neural_network <span class="keyword">as</span> sk_nn</div><div class="line">model = sk_nn.MLPClassifier(activation=<span class="string">'tanh'</span>,solver=<span class="string">'adam'</span>,alpha=<span class="number">0.0001</span>,learning_rate=<span class="string">'adaptive'</span>,learning_rate_init=<span class="number">0.001</span>,max_iter=<span class="number">200</span>)</div><div class="line">model.fit(X_train,y_train)</div><div class="line">acc=model.score(X_test,y_test) <span class="comment">#根据给定数据与标签返回正确率的均值</span></div><div class="line">print(<span class="string">'神经网络模型评价:'</span>,acc)</div></pre></td></tr></table></figure>
<p>参数说明：<br>    hidden_layer_sizes: 元祖<br>    activation：激活函数 {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认 ‘relu’<br>    solver ：优化算法{‘lbfgs’, ‘sgd’, ‘adam’}<br>    alpha：L2惩罚(正则化项)参数<br>    learning_rate：学习率 {‘constant’, ‘invscaling’, ‘adaptive’}<br>    learning_rate_init：初始学习率，默认0.001<br>    max_iter：最大迭代次数 默认200</p>
<p>特别：<br>学习率中参数：<br>      constant: 有‘learning_rate_init’给定的恒定学习率<br>      incscaling：随着时间t使用’power_t’的逆标度指数不断降低学习率<br>      adaptive：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变<br>优化算法参数：<br>      lbfgs：quasi-Newton方法的优化器<br>      sgd：随机梯度下降<br>      adam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器 </p>
<p>打印结果：（神经网络的确牛逼）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">神经网络模型评价: <span class="number">0.980952380952</span></div></pre></td></tr></table></figure>
<h3 id="KNN（K-近邻算法）"><a href="#KNN（K-近邻算法）" class="headerlink" title="KNN（K-近邻算法）"></a>KNN（K-近邻算法）</h3><p>KNN可以说是非常好用，也非常常用的分类算法了，也是最简单易懂的机器学习算法，没有之一。由于算法先天优势，KNN甚至不需要训练就可以得到非常好的分类效果了。<br>在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类。</p>
<p>其算法的描述为：<br>1.计算测试数据与各个训练数据之间的距离；<br>2.按照距离的递增关系进行排序；<br>3.选取距离最小的K个点；<br>4.确定前K个点所在类别的出现频率；<br>5.返回前K个点中出现频率最高的类别作为测试数据的预测分类。<br>(感觉又说多了…… - -!)<br>其实这个算法自己实现起来也就只有几行代码，这里我们还是使用sklearn来实现。<br>sklearn中的KNN可以做分类也可以做回归</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.neighbors <span class="keyword">as</span> sk_neighbors</div><div class="line"><span class="comment">#KNN分类</span></div><div class="line">model = sk_neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>,n_jobs=<span class="number">1</span>) </div><div class="line">model.fit(X_train,y_train)</div><div class="line">acc=model.score(X_test,y_test) <span class="comment">#根据给定数据与标签返回正确率的均值</span></div><div class="line">print(<span class="string">'KNN模型(分类)评价:'</span>,acc)</div><div class="line"></div><div class="line"><span class="comment">#KNN回归</span></div><div class="line">model = sk_neighbors.KNeighborsRegressor(n_neighbors=<span class="number">5</span>,n_jobs=<span class="number">1</span>) </div><div class="line">model.fit(X_train,y_train)</div><div class="line">acc=model.score(X_test,y_test) <span class="comment">#返回预测的确定系数R2</span></div><div class="line">print(<span class="string">'KNN模型(回归)评价:'</span>,acc)</div></pre></td></tr></table></figure>
<p>参数说明：<br>    n_neighbors： 使用邻居的数目<br>    n_jobs：并行任务数</p>
<p>打印结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">KNN模型(分类)评价: <span class="number">0.942857142857</span></div><div class="line">KNN模型(回归)评价: <span class="number">0.926060606061</span></div></pre></td></tr></table></figure>
<h2 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h2><p>好的，终于说完了常用模型，感觉完全是一个算法概述啊hhhhh<br>既然我们现在已经完成了数据的获取，模型的建立，那么最后一步便是验证我们的模型<br>其实交叉验证应该放在数据集的划分那里，但是他又与模型的验证紧密相关，所以我就按照编写代码的顺序进行讲解了。<br>首先，什么是交叉验证？<br>这里完全引用西瓜书，因为我觉得书上写的非常清楚！！！<br>交叉验证法先将数据集D划分为k个大小相似的互斥子集，每个子集Di都尽可能保持数据分布的一致性，即从D中通过分层采样得到。然后每次用k-1个子集的并集做为训练集，余下的子集做为测试集，这样就可以获得K组训练/测试集，从而可以进行k次训练和测试，最终返回的是这个k个测试结果的均值。k通常的取值是10，其他常用取值为2，5，20等。</p>
<p>这里使用KNN做为训练模型，采用十折交叉验证。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">model = sk_neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>,n_jobs=<span class="number">1</span>) <span class="comment">#KNN分类</span></div><div class="line"><span class="keyword">import</span> sklearn.model_selection <span class="keyword">as</span> sk_model_selection</div><div class="line">accs=sk_model_selection.cross_val_score(model, iris_X, y=iris_y, scoring=<span class="keyword">None</span>,cv=<span class="number">10</span>, n_jobs=<span class="number">1</span>)</div><div class="line">print(<span class="string">'交叉验证结果:'</span>,accs)</div></pre></td></tr></table></figure>
<p>参数说明：<br>model：拟合数据的模型<br>cv ： 子集个数 就是k<br>scoring: 打分参数 默认‘accuracy’、可选‘f1’、‘precision’、‘recall’ 、‘roc_auc’、’neg_log_loss’</p>
<p>打印结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">交叉验证结果:</div><div class="line"> [ <span class="number">1.</span>          <span class="number">0.93333333</span>  <span class="number">1.</span>          <span class="number">1.</span>          <span class="number">0.86666667</span>  <span class="number">0.93333333</span></div><div class="line">  <span class="number">0.93333333</span>  <span class="number">1.</span>          <span class="number">1.</span>          <span class="number">1.</span>        ]</div></pre></td></tr></table></figure>
<h2 id="模型的保存和载入"><a href="#模型的保存和载入" class="headerlink" title="模型的保存和载入"></a>模型的保存和载入</h2><p>模型的保存和载入方便我们将训练好的模型保存在本地或发送在网上，载入模型方便我们在不同的环境下进行测试。<br>使用pickle可以进行保存与载入<br>也可以使用sklearn自带的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sklearn.externals <span class="keyword">as</span> sk_externals</div><div class="line">sk_externals.joblib.dump(model,<span class="string">'model.pickle'</span>) <span class="comment">#保存</span></div><div class="line">model = sk_externals.joblib.load(<span class="string">'model.pickle'</span>) <span class="comment">#载入</span></div></pre></td></tr></table></figure>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>两篇帖子基本完成了对于sklearn的基础讲解，看似内容虽多，但是使用起来其实非常简单。不小心写成一个算法概述，也没什么太大的问题，相信我不写，大家也会去百度这些算法的含义，我就当这是为大家省时间了吧哈哈哈。<br>sklearn是一个非常好用的机器学习工具包，掌握好它会在机器学习的道路上祝我们一臂之力的！<br>与君共勉！</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/sklearn/" rel="tag"># sklearn</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/02/05/十分钟上手sklearn-1/" rel="next" title="十分钟上手sklearn：安装，获取数据，数据预处理">
                <i class="fa fa-chevron-left"></i> 十分钟上手sklearn：安装，获取数据，数据预处理
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/02/07/NumPyQuickStart/" rel="prev" title="NumPy快速上手指南">
                NumPy快速上手指南 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Black'Blog</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#特征提取"><span class="nav-number">1.</span> <span class="nav-text">特征提取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PCA（主成分分析）"><span class="nav-number">1.1.</span> <span class="nav-text">PCA（主成分分析）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LDA（线性评价分析）"><span class="nav-number">1.2.</span> <span class="nav-text">LDA（线性评价分析）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用模型"><span class="nav-number">2.</span> <span class="nav-text">常用模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性回归"><span class="nav-number">2.1.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑回归"><span class="nav-number">2.2.</span> <span class="nav-text">逻辑回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#朴素贝叶斯"><span class="nav-number">2.3.</span> <span class="nav-text">朴素贝叶斯</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树"><span class="nav-number">2.4.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVM-支持向量机）"><span class="nav-number">2.5.</span> <span class="nav-text">SVM(支持向量机）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络"><span class="nav-number">2.6.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KNN（K-近邻算法）"><span class="nav-number">2.7.</span> <span class="nav-text">KNN（K-近邻算法）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉验证"><span class="nav-number">3.</span> <span class="nav-text">交叉验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型的保存和载入"><span class="nav-number">4.</span> <span class="nav-text">模型的保存和载入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">5.</span> <span class="nav-text">小结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Black'Blog</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4
| <span>Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></span>
</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: true,
        notify: false,
        appId: 'IWsmYIsP7eKH7JUiJa3GCocc-gzGzoHsz',
        appKey: 'q4Y9saqMlAIFYHVDXQQRa5sI',
        placeholder: '小哥，不说两句？',
        avatar:'wavatar',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("IWsmYIsP7eKH7JUiJa3GCocc-gzGzoHsz", "q4Y9saqMlAIFYHVDXQQRa5sI");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  

  

  

</body>
</html>
